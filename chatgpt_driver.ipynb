{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium.webdriver.remote.webdriver import By\n",
    "import undetected_chromedriver as uc\n",
    "import time\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = uc.Chrome()\n",
    "driver.get(\"https://chat.openai.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sleep(sec):\n",
    "    time.sleep(sec + random.randint(0,20) / 10)\n",
    "\n",
    "with open(\"./sets/qu_to_do.json\", encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# print(data[0][\"questions\"][0])\n",
    "\n",
    "#df = pd.read_json(\"./data/database.json\")\n",
    "#print(df[0])\n",
    "#save = {\"q\": [], \"qs\": []}\n",
    "PROMPT = \"\"\"Rephrase the following question in 5 different ways, and print out the result in a list form. Only give the results, no other text is wanted: {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(n):\n",
    "    xpath = f'//*[@id=\"__next\"]/div[2]/div[2]/div/main/div[2]/div/div/div/div[{n*2}]/div'\n",
    "    return driver.find_element(By.XPATH, xpath).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Can I buy statins over the counter?\n",
      "1 : How else can I lower my cholesterol?\n",
      "2 : What is TAVI ?\n",
      "3 : What is a TAVI?\n",
      "4 : Why would you need a TAVI?\n",
      "5 : How successful are TAVI procedures?\n",
      "6 : How long does a TAVI heart valve last?\n",
      "7 : What are the benefits of TAVI?\n",
      "8 : What are the risks of TAVI?\n",
      "9 : What happens during a TAVI?\n",
      "10 : How to prepare before a TAVI procedure?\n",
      "11 : What is TAVI recovery time and what to expect after a TAVI procedure ?\n",
      "12 : When can you exercise after a TAVI procedure?\n",
      "13 : How long after a TAVI procedure can you drive?\n",
      "14 : How soon can you fly after having a TAVI procedure?\n",
      "15 : When can I return to work after a TAVI procedure?\n",
      "16 : Antibiotic cover for dental surgery after a TAVI?\n",
      "17 : why you should do a Tilt test ?\n",
      "18 : What can the Tilt test show?\n",
      "19 : What happens during a tilt test?\n",
      "20 : How can I prepare for a tilt test?\n",
      "21 : Whatâ€™s the treatment for angina?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = 0\n",
    "for n, elem in enumerate(data[330:-1]):\n",
    "    print(f\"{n} : \" + elem[\"questions\"][0])\n",
    "    driver.find_element(\n",
    "        By.XPATH, '//*[@id=\"prompt-textarea\"]').send_keys(PROMPT.format(elem[\"questions\"][0]))\n",
    "    sleep(random.randint(5, 10))\n",
    "    questions = get_answer(n+1).split(\"\\n\")\n",
    "    for q in questions: elem[\"questions\"].append(q)\n",
    "    sleep(1)\n",
    "    n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "elem = data[52]\n",
    "print(elem[\"questions\"][0])\n",
    "driver.find_element(\n",
    "    By.XPATH, '//*[@id=\"prompt-textarea\"]').send_keys(PROMPT.format(elem[\"questions\"][0]))\n",
    "sleep(random.randint(5, 10))\n",
    "questions = get_answer(n+1).split(\"\\n\")\n",
    "for q in questions:\n",
    "    elem[\"questions\"].append(q)\n",
    "sleep(1)\n",
    "n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0][\"questions\"][0])\n",
    "driver.find_element(\n",
    "    By.XPATH, '//*[@id=\"prompt-textarea\"]').send_keys(PROMPT.format(data[0][\"questions\"][0]))\n",
    "sleep(10)\n",
    "questions = get_answer(4+1).split(\"\\n\")\n",
    "for q in questions: data[0][\"questions\"].append(q)\n",
    "sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[329][\"questions\"])\n",
    "print(data[330][\"questions\"])\n",
    "print(data[327][\"questions\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25988"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = open(\"./sets/newquestions.json\", \"a\", encoding=\"utf8\")\n",
    "out.write(json.dumps(data[330:-1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_questions():\n",
    "    with open(\"./sets/data_done.json\", encoding=\"utf8\") as f:\n",
    "        f_o = json.load(f)\n",
    "    with open(\"./sets/qu_to_do.json\", encoding=\"utf8\") as f:\n",
    "        f_n = json.load(f)\n",
    "    for n, elem_o in enumerate(f_o):\n",
    "        count = 0\n",
    "        for elem_n in f_n:\n",
    "            if elem_o[\"questions\"][0] == elem_n[\"questions\"][0]:\n",
    "                count += 1\n",
    "        if count == 0:\n",
    "            print(elem_o[\"questions\"][0])\n",
    "            print(n)\n",
    "\n",
    "compare_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def find_anomalies(delete: bool = False):\n",
    "    error_questions = []\n",
    "    dup_count = 0\n",
    "    err_count = 0\n",
    "    with open(\"./data/result.json\", encoding=\"utf8\") as f:\n",
    "        f_o = json.load(f)\n",
    "    #with open(\"./sets/newquestions.json\", encoding=\"utf8\") as f:\n",
    "        #f_n = json.load(f)\n",
    "    for n, elem_o in enumerate(f_o):\n",
    "        count = 0\n",
    "        for m, elem_o2 in enumerate(f_o):\n",
    "            if elem_o[\"q_EN\"] == elem_o2[\"q_EN\"]:\n",
    "                count += 1\n",
    "                if count > 1:\n",
    "                    if m != n :\n",
    "                        print(f'{m} <=> {n} : {elem_o[\"q_EN\"]}')\n",
    "                        dup_count += 1\n",
    "                        if delete : del f_o[m]\n",
    "        if len(elem_o[\"qs_FR\"]) < 5:\n",
    "            print(f'{n} : missing questions : {elem_o[\"qs_FR\"][0]}')\n",
    "        for l, q in enumerate(elem_o[\"qs_FR\"]):\n",
    "            if len(q) < 12:\n",
    "                print(f'{n} : question error : {q}')\n",
    "                error_questions.append(n)\n",
    "                err_count+=1\n",
    "    \n",
    "    print(f'Duplicate count : {dup_count}')\n",
    "    print(f'Error count : {err_count}')\n",
    "\n",
    "    out = open(\"./data/final2.json\", \"a\", encoding=\"utf8\")\n",
    "    out.write(json.dumps(f_o))\n",
    "\n",
    "find_anomalies(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564 missing : How to live with vascular dementia?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def find_missing_elem():\n",
    "    with open(\"./data/result.json\", encoding=\"utf8\") as f:\n",
    "        f_n = json.load(f)\n",
    "    with open(\"./data/dataset.json\", encoding=\"utf8\") as f:\n",
    "        f_o = json.load(f)\n",
    "    for n, elem_o in enumerate(f_o):\n",
    "        count = 0\n",
    "        for m, elem_n in enumerate(f_n):\n",
    "            if elem_o[\"q_EN\"] == elem_n[\"q_EN\"]:\n",
    "                count += 1\n",
    "        if count == 0:\n",
    "            print(f'{n} missing : {elem_o[\"q_EN\"]}')\n",
    "\n",
    "    # out = open(\"./data/final2.json\", \"a\", encoding=\"utf8\")\n",
    "    # out.write(json.dumps(f_o))\n",
    "\n",
    "find_missing_elem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./sets/data_q_only.json\", encoding=\"utf8\") as f:\n",
    "    f_all = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in f_all[\"questions\"]:\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def new_file_without_dup():\n",
    "    with open(\"./sets/data_q_only.json\", encoding=\"utf8\") as f:\n",
    "        f_all = json.load(f)\n",
    "    with open(\"./sets/5q_and_answer.json\", encoding=\"utf8\") as f:\n",
    "        f_done = json.load(f)\n",
    "    with open(\"./sets/message.txt\", encoding=\"utf8\") as f:\n",
    "        message = f.read().split(\"\\n\")\n",
    "\n",
    "    max_index = 573\n",
    "\n",
    "    for i in range(0, max_index):\n",
    "        for line in message:\n",
    "            line = line.strip()\n",
    "            #print(f_all[\"questions\"]['{index}'.format(index=i)])\n",
    "            index = f'{i}'\n",
    "            txt = f_all[\"questions\"][index]\n",
    "            if txt.find(line) != -1:\n",
    "                print(txt)\n",
    "                del f_all[\"questions\"]['{index}'.format(index=i)]\n",
    "                continue\n",
    "            else:\n",
    "                for n, elem_done in enumerate(f_done):\n",
    "                    if f_all[\"questions\"]['{index}'.format(index=i)].find(f_done[\"questions\"][0].strip()) != -1:\n",
    "                        print(f_all[\"questions\"]['{index}'.format(index=i)])\n",
    "                        del f_all[\"questions\"]['{index}'.format(index=i)]\n",
    "                continue\n",
    "\n",
    "new_file_without_dup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./sets/data_q_only.json\", encoding=\"utf8\") as f:\n",
    "    f_all = json.load(f)\n",
    "print(f_all[\"questions\"][f'{0}'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open(\"./set/qu_to_do.json\", \"a\", encoding=\"utf8\")\n",
    "out.write(json.dumps(f_all))  # Always add +1 to the last index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Regarder si la question est dans `message.txt` ou `\n",
    "* Si non, envoyer dans ChatGPT\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn given file in usable `.json` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Turn `data_leo.json` into real json\n",
    "jstring = []\n",
    "\n",
    "with open(\"./sets/data_leo.json\", encoding=\"utf8\") as f:\n",
    "    f_leo = json.load(f)\n",
    "\n",
    "for e_f_leo in f_leo[\"questions\"]:\n",
    "    question = f_leo[\"questions\"][e_f_leo]\n",
    "    definition = f_leo[\"answers\"][e_f_leo]\n",
    "\n",
    "    jstring.append({\"questions\": [], \"definition\": \"\"})\n",
    "    jstring[-1][\"questions\"].append(question)\n",
    "    jstring[-1][\"definition\"] = definition\n",
    "\n",
    "out = open(\"./sets/qu_to_do.json\", \"a\", encoding=\"utf8\")\n",
    "out.write(json.dumps(jstring))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./sets/data_leo_tojs.json\", encoding=\"utf8\") as f:\n",
    "    f_leo = json.load(f)\n",
    "with open(\"./sets/data_done.json\", encoding=\"utf8\") as f:\n",
    "    f_done = json.load(f)\n",
    "with open(\"./sets/message.txt\", encoding=\"utf8\") as f:\n",
    "    f_mess = f.read().split(\"\\n\")\n",
    "\n",
    "for e_f_done in f_done:\n",
    "    for n_f_leo, e_f_leo in enumerate(f_leo):\n",
    "        if e_f_leo[\"questions\"][0].find(e_f_done[\"questions\"][0].strip()) != -1:\n",
    "            # print(n_f_leo)\n",
    "            print(e_f_leo[\"questions\"][0].strip())\n",
    "            del f_leo[n_f_leo]\n",
    "\n",
    "for e_f_mess in f_mess:\n",
    "    for n_f_leo, e_f_leo in enumerate(f_leo):\n",
    "        if e_f_leo[\"questions\"][0].find(e_f_mess.strip()) != -1:\n",
    "            #print(n_f_leo)\n",
    "            print(e_f_leo[\"questions\"][0].strip())\n",
    "            del f_leo[n_f_leo]\n",
    "\n",
    "out = open(\"./sets/qu_to_do.json\", \"a\", encoding=\"utf8\")\n",
    "out.write(json.dumps(f_leo))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_questions(path: str):\n",
    "    jstring = []\n",
    "    with open(path, encoding=\"utf8\") as o:\n",
    "        f = o.read()\n",
    "        df = f.split(\"\\n\\n\\n\")\n",
    "        del df[0]\n",
    "        for df_string in df:\n",
    "            if (df_string[0] == ' ' or df_string[0] == '\\n') : break\n",
    "            subtext = df_string.split(\"\\n\")\n",
    "            jstring.append({\"questions\": [], \"definition\" : \"\"})\n",
    "            jstring[-1][\"questions\"].append(subtext[0])\n",
    "            definition = \"\"\n",
    "            for t in subtext[1:-1] : \n",
    "                definition += \" \" + t \n",
    "            jstring[-1][\"definition\"] = definition\n",
    "    print(jstring)\n",
    "    out = open(\"./data/newdata.json\", \"a\", encoding=\"utf8\")\n",
    "    out.write(json.dumps(jstring))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_questions(\"./data/angina_-_cause_symptoms_treatments.txt\")\n",
    "\n",
    "import os\n",
    "\n",
    "path = \"./data_texts/\"\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        parse_questions(path + filename)\n",
    "        continue\n",
    "    else:\n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
