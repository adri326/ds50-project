{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade deepl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import deepl\n",
    "import os\n",
    "\n",
    "auth_key = \"f59740cb-63df-4b86-fcb0-ce345faca270:fx\"\n",
    "\n",
    "trsl = deepl.Translator(auth_key=auth_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"./dataset.json\")\n",
    "\n",
    "saved_q = {\"EN\": [], \"FR\": []}\n",
    "for i, a in enumerate(df.qs_EN):\n",
    "    saved_q[\"EN\"].append(a)\n",
    "    saved_q[\"FR\"].append(translator.translate_text(\n",
    "        \"\\n\".join(a), source_lang=\"EN\", target_lang=\"FR\").text.split(\"\\n\"))\n",
    "    print(f\"{i+1}/{len(df.questions)}\", end=\"\\r\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/dataset.json\", encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "start = 300\n",
    "end = -1\n",
    "\n",
    "data_range = data[start:end]\n",
    "\n",
    "questions_fr = []\n",
    "for n, elem in enumerate(data_range):\n",
    "    del questions_fr\n",
    "    questions_fr = []\n",
    "\n",
    "    for m, q in enumerate(elem[\"qs_EN\"]):\n",
    "        questions_fr.append(trsl.translate_text(q, source_lang=\"EN\", target_lang=\"FR\").text)\n",
    "    data_range[n][\"qs_FR\"] = questions_fr\n",
    "\n",
    "    print(\"-\", end='')\n",
    "print(\"Translation finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/dataset.json\", encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "elem = data[564]\n",
    "\n",
    "questions_fr = []\n",
    "\n",
    "for m, q in enumerate(elem[\"qs_EN\"]):\n",
    "    questions_fr.append(trsl.translate_text(q, source_lang=\"EN\", target_lang=\"FR\").text)\n",
    "elem[\"qs_FR\"] = questions_fr\n",
    "\n",
    "print(\"-\", end='')\n",
    "print()\n",
    "print(\"Translation finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open(\"./data/final2.json\", \"a\", encoding=\"utf8\")\n",
    "out.write(json.dumps(elem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, elem in enumerate(data[1:50]):\n",
    "    print(\"n : \" + elem[\"qs_FR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for letter in \"Je te Veux\": print(ord(letter))\n",
    "\"\".translate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def does_contain(dataset: list, word: str):\n",
    "    for bound in dataset:\n",
    "        for elem in bound:\n",
    "            if elem == word:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acronym count : 86\n",
      "['AAA']\n",
      "['dAAA']\n",
      "['MRI', 'IRM']\n",
      "['EVAR']\n",
      "['SVT', 'TSV']\n",
      "['CPVT', 'PCCD']\n",
      "['ECG']\n",
      "['ICD']\n",
      "['DVLA']\n",
      "['ACM', 'MCA']\n",
      "['ARVC', 'ARVD']\n",
      "['lACM']\n",
      "['DAI']\n",
      "['SCD', 'MSC']\n",
      "['AVC']\n",
      "['NOACs', 'NOAC']\n",
      "['ECGs', 'dECG']\n",
      "['CPR']\n",
      "['RCP']\n",
      "['DCI']\n",
      "['NHS']\n",
      "['HCM', 'DCM', 'RCM', 'LVNC', 'PPCM', 'CMH', 'CMD', 'CVDA', 'CPPM']\n",
      "['ICDs', 'SADS']\n",
      "['CVD', 'MCV']\n",
      "['TVPC']\n",
      "['LDL']\n",
      "['DNA', 'dADN']\n",
      "['PCSK9']\n",
      "['CCU', 'USC']\n",
      "['ACE', 'ARBs', 'GTN', 'lECA', 'ARA']\n",
      "['CHD']\n",
      "['PHA']\n",
      "['BNP']\n",
      "['HFpEF', 'ICFpE', 'ICFP']\n",
      "['LQTS', 'STLQ']\n",
      "['lHCM']\n",
      "['HGV']\n",
      "['SQTL']\n",
      "['SQFT']\n",
      "['RPC']\n",
      "['HRT', 'DVT', 'THS', 'TVP']\n",
      "['BHFfunded', 'lIRM', 'BHF']\n",
      "['DPC']\n",
      "['CCPD', 'DCPC']\n",
      "['DCP']\n",
      "['GIS']\n",
      "['CRTD']\n",
      "['SCAD']\n",
      "['WPW']\n",
      "['lECG']\n",
      "['THR', 'FCC']\n",
      "['FBC', 'NFS', 'testsBNP']\n",
      "['LIRM']\n",
      "['dIRM']\n",
      "['LECG']\n",
      "['ILR']\n",
      "['TTE', 'ETT']\n",
      "['TOE', 'TIA', 'ETO']\n",
      "['IHC']\n",
      "['ILRs']\n",
      "['dILR']\n",
      "['MRIs']\n",
      "['MIBI']\n",
      "['MPS']\n",
      "['LVAD', 'DAVG']\n",
      "['TAVI']\n",
      "['TIAs', 'AIT', 'dAVC']\n",
      "['SICD', 'DAISIC']\n",
      "['TENS']\n",
      "['ARNi']\n",
      "['CRT']\n",
      "['CAT']\n",
      "['HDL', 'nonHDL']\n",
      "['TAVR', 'USA']\n",
      "['PAD', 'miniAVC', 'MAP']\n",
      "['PCI', 'ICP']\n",
      "['FITT']\n",
      "['BMI', 'IMC']\n",
      "['NMS']\n",
      "['ABPM', 'HBPM', 'MAPA']\n",
      "['NonHDL']\n",
      "['CMI']\n",
      "['dIMC', 'LIMC']\n",
      "['FAST']\n",
      "['TIA’s']\n",
      "['lAVC', 'LAVC']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "def find_acronyms():\n",
    "    with open(\"./data/final2.json\", encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "    str_indices = [\"q_EN\", \"a_EN\", \"q_FR\", \"a_FR\"]\n",
    "    lst_indices = [\"qs_EN\", \"qs_FR\"]\n",
    "\n",
    "    acronyms = []\n",
    "    \n",
    "    for num, topic in enumerate(data):\n",
    "\n",
    "        bound = []\n",
    "\n",
    "        for index in str_indices:\n",
    "            batch = topic[index].translate(\n",
    "                str.maketrans('', '', string.punctuation))\n",
    "            words = batch.split()\n",
    "            for word in words:\n",
    "                count = 0\n",
    "                if len(word) < 10:\n",
    "                    for letter in word: \n",
    "                        if ord(letter) > 64 and ord(letter) < 91 : count += 1\n",
    "                    if count > 2 : \n",
    "                        if not does_contain(acronyms, word) and word not in bound: bound.append(word)\n",
    "\n",
    "        for index in lst_indices:\n",
    "            for n, lst_item in enumerate(topic[index]):\n",
    "                batch = lst_item.translate(\n",
    "                    str.maketrans('', '', string.punctuation))\n",
    "                words = batch.split()\n",
    "                for word in words:\n",
    "                    count = 0\n",
    "                    if len(word) < 10:\n",
    "                        for letter in word:\n",
    "                            if ord(letter) > 64 and ord(letter) < 91:\n",
    "                                count += 1\n",
    "                        if count > 2:\n",
    "                            if not does_contain(acronyms, word) and word not in bound:\n",
    "                                bound.append(word)\n",
    "        \n",
    "        if len(bound) != 0:\n",
    "            acronyms.append(bound)\n",
    "        del bound\n",
    "\n",
    "    print(f'Acronym count : {len(acronyms)}')\n",
    "    for word in acronyms: print(word)\n",
    "    out = open(\"./data/aconyms.json\", \"a\", encoding=\"utf8\")\n",
    "    out.write(json.dumps(acronyms))\n",
    "\n",
    "find_acronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acronym count : 86\n",
      "['AAA']\n",
      "['dAAA']\n",
      "['MRI', 'IRM']\n",
      "['EVAR']\n",
      "['SVT', 'TSV']\n",
      "['CPVT', 'PCCD']\n",
      "['ECG']\n",
      "['ICD']\n",
      "['DVLA']\n",
      "['ACM', 'MCA']\n",
      "['ARVC', 'ARVD']\n",
      "['lACM']\n",
      "['DAI']\n",
      "['SCD', 'MSC']\n",
      "['AVC']\n",
      "['NOACs', 'NOAC']\n",
      "['ECGs', 'dECG']\n",
      "['CPR']\n",
      "['RCP']\n",
      "['DCI']\n",
      "['NHS']\n",
      "['HCM', 'DCM', 'RCM', 'LVNC', 'PPCM', 'CMH', 'CMD', 'CVDA', 'CPPM']\n",
      "['ICDs', 'SADS']\n",
      "['CVD', 'MCV']\n",
      "['TVPC']\n",
      "['LDL']\n",
      "['DNA', 'dADN']\n",
      "['PCSK9']\n",
      "['CCU', 'USC']\n",
      "['ACE', 'ARBs', 'GTN', 'lECA', 'ARA']\n",
      "['CHD']\n",
      "['PHA']\n",
      "['BNP']\n",
      "['HFpEF', 'ICFpE', 'ICFP']\n",
      "['LQTS', 'STLQ']\n",
      "['lHCM']\n",
      "['HGV']\n",
      "['SQTL']\n",
      "['SQFT']\n",
      "['RPC']\n",
      "['HRT', 'DVT', 'THS', 'TVP']\n",
      "['BHFfunded', 'lIRM', 'BHF']\n",
      "['DPC']\n",
      "['CCPD', 'DCPC']\n",
      "['DCP']\n",
      "['GIS']\n",
      "['CRTD']\n",
      "['SCAD']\n",
      "['WPW']\n",
      "['lECG']\n",
      "['THR', 'FCC']\n",
      "['FBC', 'NFS', 'testsBNP']\n",
      "['LIRM']\n",
      "['dIRM']\n",
      "['LECG']\n",
      "['ILR']\n",
      "['TTE', 'ETT']\n",
      "['TOE', 'TIA', 'ETO']\n",
      "['IHC']\n",
      "['ILRs']\n",
      "['dILR']\n",
      "['MRIs']\n",
      "['MIBI']\n",
      "['MPS']\n",
      "['LVAD', 'DAVG']\n",
      "['TAVI']\n",
      "['TIAs', 'AIT', 'dAVC']\n",
      "['SICD', 'DAISIC']\n",
      "['TENS']\n",
      "['ARNi']\n",
      "['CRT']\n",
      "['CAT']\n",
      "['HDL', 'nonHDL']\n",
      "['TAVR', 'USA']\n",
      "['PAD', 'miniAVC', 'MAP']\n",
      "['PCI', 'ICP']\n",
      "['FITT']\n",
      "['BMI', 'IMC']\n",
      "['NMS']\n",
      "['ABPM', 'HBPM', 'MAPA']\n",
      "['NonHDL']\n",
      "['CMI']\n",
      "['dIMC', 'LIMC']\n",
      "['FAST']\n",
      "['TIA’s']\n",
      "['lAVC', 'LAVC']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "\n",
    "def count_acronyms():\n",
    "    with open(\"./data/aconyms.json\", encoding=\"utf8\") as f:\n",
    "        acr = json.load(f)\n",
    "    with open(\"./data/aconyms.json\", encoding=\"utf8\") as f:\n",
    "        data = f.read()\n",
    "    \n",
    "    data = data.translate(\n",
    "        str.maketrans('', '', string.punctuation))\n",
    "    data = data.split()\n",
    "\n",
    "    for elem in data:\n",
    "        \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 256):\n",
    "    print(f'{i} + {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"./data/final2.json\", encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_range)\n",
    "new_questions = []\n",
    "for elem in data_range[0][\"qs_FR\"]:\n",
    "    new_questions.append(elem.text)\n",
    "data_range[0][\"qs_FR\"] = new_questions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open(\"./data/result.json\", \"a\", encoding=\"utf8\")\n",
    "out.write(json.dumps(data_range))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepl import GlossaryInfo\n",
    "\n",
    "glossary = GlossaryInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = trsl.translate_text(\"Do I need to get an appointment with my GP?\", source_lang=\"EN\", target_lang=\"FR\")\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
